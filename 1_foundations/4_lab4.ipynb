{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first big project - Professionally You!\n",
    "\n",
    "### And, Tool use.\n",
    "\n",
    "### But first: introducing Pushover\n",
    "\n",
    "Pushover is a nifty tool for sending Push Notifications to your phone.\n",
    "\n",
    "It's super easy to set up and install!\n",
    "\n",
    "Simply visit https://pushover.net/ and click 'Login or Signup' on the top right to sign up for a free account, and create your API keys.\n",
    "\n",
    "Once you've signed up, on the home screen, click \"Create an Application/API Token\", and give it any name (like Agents) and click Create Application.\n",
    "\n",
    "Then add 2 lines to your `.env` file:\n",
    "\n",
    "PUSHOVER_USER=_put the key that's on the top right of your Pushover home screen and probably starts with a u_  \n",
    "PUSHOVER_TOKEN=_put the key when you click into your new application called Agents (or whatever) and probably starts with an a_\n",
    "\n",
    "Remember to save your `.env` file, and run `load_dotenv(override=True)` after saving, to set your environment variables.\n",
    "\n",
    "Finally, click \"Add Phone, Tablet or Desktop\" to install on your phone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr\n",
    "import sqlite3\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# RAG dependencies - install if needed\n",
    "try:\n",
    "    import faiss\n",
    "except ImportError:\n",
    "    print(\"Installing faiss-cpu...\")\n",
    "    os.system(\"uv add faiss-cpu\")\n",
    "    import faiss\n",
    "\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi\n",
    "except ImportError:\n",
    "    print(\"Installing rank-bm25...\")\n",
    "    os.system(\"uv add rank-bm25\")\n",
    "    from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usual start\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushover user found and starts with u\n",
      "Pushover token found and starts with a\n"
     ]
    }
   ],
   "source": [
    "# For pushover\n",
    "\n",
    "pushover_user = os.getenv(\"PUSHOVER_USER\")\n",
    "pushover_token = os.getenv(\"PUSHOVER_TOKEN\")\n",
    "pushover_url = \"https://api.pushover.net/1/messages.json\"\n",
    "\n",
    "if pushover_user:\n",
    "    print(f\"Pushover user found and starts with {pushover_user[0]}\")\n",
    "else:\n",
    "    print(\"Pushover user not found\")\n",
    "\n",
    "if pushover_token:\n",
    "    print(f\"Pushover token found and starts with {pushover_token[0]}\")\n",
    "else:\n",
    "    print(\"Pushover token not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push(message):\n",
    "    print(f\"Push: {message}\")\n",
    "    payload = {\"user\": pushover_user, \"token\": pushover_token, \"message\": message}\n",
    "    requests.post(pushover_url, data=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Push: HEY!!\n"
     ]
    }
   ],
   "source": [
    "push(\"HEY!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_user_details(email, name=\"Name not provided\", notes=\"not provided\"):\n",
    "    push(f\"Recording interest from {name} with email {email} and notes {notes}\")\n",
    "    return {\"recorded\": \"ok\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_unknown_question(question):\n",
    "    push(f\"Recording {question} asked that I couldn't answer\")\n",
    "    return {\"recorded\": \"ok\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_user_details_json = {\n",
    "    \"name\": \"record_user_details\",\n",
    "    \"description\": \"Use this tool to record that a user is interested in being in touch and provided an email address\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"email\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The email address of this user\"\n",
    "            },\n",
    "            \"name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The user's name, if they provided it\"\n",
    "            }\n",
    "            ,\n",
    "            \"notes\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Any additional information about the conversation that's worth recording to give context\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"email\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_unknown_question_json = {\n",
    "    \"name\": \"record_unknown_question\",\n",
    "    \"description\": \"Always use this tool to record any question that couldn't be answered as you didn't know the answer\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The question that couldn't be answered\"\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"question\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\"type\": \"function\", \"function\": record_user_details_json},\n",
    "        {\"type\": \"function\", \"function\": record_unknown_question_json}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'record_user_details',\n",
       "   'description': 'Use this tool to record that a user is interested in being in touch and provided an email address',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'email': {'type': 'string',\n",
       "      'description': 'The email address of this user'},\n",
       "     'name': {'type': 'string',\n",
       "      'description': \"The user's name, if they provided it\"},\n",
       "     'notes': {'type': 'string',\n",
       "      'description': \"Any additional information about the conversation that's worth recording to give context\"}},\n",
       "    'required': ['email'],\n",
       "    'additionalProperties': False}}},\n",
       " {'type': 'function',\n",
       "  'function': {'name': 'record_unknown_question',\n",
       "   'description': \"Always use this tool to record any question that couldn't be answered as you didn't know the answer\",\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'question': {'type': 'string',\n",
       "      'description': \"The question that couldn't be answered\"}},\n",
       "    'required': ['question'],\n",
       "    'additionalProperties': False}}}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function can take a list of tool calls, and run them. This is the IF statement!!\n",
    "\n",
    "def handle_tool_calls(tool_calls):\n",
    "    results = []\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call.function.name\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        print(f\"Tool called: {tool_name}\", flush=True)\n",
    "\n",
    "        # THE BIG IF STATEMENT!!!\n",
    "\n",
    "        if tool_name == \"record_user_details\":\n",
    "            result = record_user_details(**arguments)\n",
    "        elif tool_name == \"record_unknown_question\":\n",
    "            result = record_unknown_question(**arguments)\n",
    "\n",
    "        results.append({\"role\": \"tool\",\"content\": json.dumps(result),\"tool_call_id\": tool_call.id})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Push: Recording this is a really hard question asked that I couldn't answer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'recorded': 'ok'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()[\"record_unknown_question\"](\"this is a really hard question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a more elegant way that avoids the IF statement.\n",
    "\n",
    "def handle_tool_calls(tool_calls):\n",
    "    results = []\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call.function.name\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        print(f\"Tool called: {tool_name}\", flush=True)\n",
    "        tool = globals().get(tool_name)\n",
    "        result = tool(**arguments) if tool else {}\n",
    "        results.append({\"role\": \"tool\",\"content\": json.dumps(result),\"tool_call_id\": tool_call.id})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 110 0 (offset 0)\n",
      "Ignoring wrong pointing object 113 0 (offset 0)\n",
      "Ignoring wrong pointing object 115 0 (offset 0)\n",
      "Ignoring wrong pointing object 117 0 (offset 0)\n",
      "Ignoring wrong pointing object 119 0 (offset 0)\n",
      "Ignoring wrong pointing object 123 0 (offset 0)\n",
      "Ignoring wrong pointing object 124 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database initialized with sample Q&A\n"
     ]
    }
   ],
   "source": [
    "reader = PdfReader(\"me/cameronbell2.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text\n",
    "\n",
    "with open(\"me/summary2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()\n",
    "\n",
    "name = \"Cameron Bell\"\n",
    "\n",
    "# Initialize SQLite database\n",
    "conn = sqlite3.connect('qa_database.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create Q&A table\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS qa (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    question TEXT UNIQUE,\n",
    "    answer TEXT,\n",
    "    category TEXT,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ")\n",
    "''')\n",
    "\n",
    "# Seed with some sample Q&A based on your profile\n",
    "sample_qa = [\n",
    "    (\"Where are you from?\", \"I'm from Southampton, Bermuda, but I've lived in England for 10 years and Madrid, Spain for a year during my MSc.\", \"background\"),\n",
    "    (\"What's your educational background?\", \"I attended Ardingly College for boarding school, completed my International Baccalaureate, then went to University of Bristol for my MSc in Computer Science and Business Technology.\", \"education\"),\n",
    "    (\"What are your hobbies?\", \"I love all foods, particularly trying new cuisine. I'm also a gym enthusiast and enjoy weights and calisthenics. I'm a big football fan too!\", \"personal\"),\n",
    "    (\"Why did you move back to Bermuda?\", \"In 2022, I returned to Bermuda to be with my ageing grandfather and support my family.\", \"background\"),\n",
    "]\n",
    "\n",
    "for q, a, cat in sample_qa:\n",
    "    cursor.execute(\"INSERT OR IGNORE INTO qa (question, answer, category) VALUES (?, ?, ?)\", (q, a, cat))\n",
    "\n",
    "conn.commit()\n",
    "print(\"Database initialized with sample Q&A\")\n",
    "\n",
    "def search_qa(user_query, limit=3):\n",
    "    \"\"\"Search for relevant Q&A based on keyword matching\"\"\"\n",
    "    keywords = re.findall(r'\\b\\w+\\b', user_query.lower())\n",
    "    keyword_pattern = '%' + '%'.join(keywords[:3]) + '%'\n",
    "    \n",
    "    cursor.execute('''\n",
    "        SELECT question, answer, category \n",
    "        FROM qa \n",
    "        WHERE question LIKE ? OR answer LIKE ?\n",
    "        LIMIT ?\n",
    "    ''', (keyword_pattern, keyword_pattern, limit))\n",
    "    \n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    if results:\n",
    "        context = \"Relevant Q&A from knowledge base:\\n\"\n",
    "        for q, a, cat in results:\n",
    "            context += f\"Q: {q}\\nA: {a}\\n\\n\"\n",
    "        return context\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Solutions - 3 Approaches\n",
    "\n",
    "Below are 3 different RAG implementations for retrieving relevant information from my documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 71 chunks from all documents\n",
      "First chunk preview: My name is Cameron Bell. I'm an AI / ML engineer, software developer and data scientist. I'm origina...\n"
     ]
    }
   ],
   "source": [
    "# Shared function: Chunk documents\n",
    "\n",
    "def chunk_documents(chunk_size=500, overlap=50):\n",
    "    \"\"\"Chunk all documents into smaller pieces for retrieval\"\"\"\n",
    "    # Read CV (already have linkedin from cameronbell2.pdf loaded above)\n",
    "    cv_reader = PdfReader(\"me/cameronbell_cv.pdf\")\n",
    "    cv = \"\"\n",
    "    for page in cv_reader.pages:\n",
    "        text = page.extract_text()\n",
    "        if text:\n",
    "            cv += text\n",
    "    \n",
    "    # Combine all text\n",
    "    all_text = summary + \"\\n\\n\" + linkedin + \"\\n\\n\" + cv\n",
    "    \n",
    "    # Split into chunks\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(all_text):\n",
    "        end = start + chunk_size\n",
    "        chunk = all_text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test chunking\n",
    "doc_chunks = chunk_documents()\n",
    "print(f\"Created {len(doc_chunks)} chunks from all documents\")\n",
    "print(f\"First chunk preview: {doc_chunks[0][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: OpenAI Embeddings + FAISS Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS vector store with OpenAI embeddings...\n",
      "Created FAISS index with 71 vectors\n",
      "Relevant chunks from knowledge base (Embeddings):\n",
      "\n",
      "1. y queries and computer \n",
      "skills. I have always had an affection for elderly citizens due to my close \n",
      "relationship with my grandparents. This was a way to help teach them key \n",
      "technological skills, to ...\n",
      "\n",
      "2. pt, with hands-on experience in RESTful APIs, full-stack \n",
      "applications, and cloud deployment AWS, GCP, Azure). Proficient with FastAPI, \n",
      "Flask, React, and CI/CD pipelines GitHub Actions, Docker). St...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution 1: OpenAI Embeddings + FAISS\n",
    "\n",
    "# Build vector store\n",
    "print(\"Building FAISS vector store with OpenAI embeddings...\")\n",
    "chunks = chunk_documents()\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = []\n",
    "for chunk in chunks:\n",
    "    response = openai.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=chunk\n",
    "    )\n",
    "    embeddings.append(response.data[0].embedding)\n",
    "\n",
    "# Convert to numpy array\n",
    "embeddings_array = np.array(embeddings).astype('float32')\n",
    "dimension = embeddings_array.shape[1]\n",
    "\n",
    "# Create FAISS index\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(embeddings_array)\n",
    "\n",
    "print(f\"Created FAISS index with {faiss_index.ntotal} vectors\")\n",
    "\n",
    "def search_embeddings(query, top_k=3):\n",
    "    \"\"\"Search using semantic embeddings\"\"\"\n",
    "    # Embed the query\n",
    "    response = openai.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=query\n",
    "    )\n",
    "    query_embedding = np.array([response.data[0].embedding]).astype('float32')\n",
    "    \n",
    "    # Search FAISS index\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Get top chunks\n",
    "    results = [chunks[idx] for idx in indices[0]]\n",
    "    \n",
    "    context = f\"Relevant chunks from knowledge base (Embeddings):\\n\\n\"\n",
    "    for i, chunk in enumerate(results, 1):\n",
    "        context += f\"{i}. {chunk[:200]}...\\n\\n\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# Test\n",
    "test_query = \"What are your technical skills?\"\n",
    "print(search_embeddings(test_query, top_k=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Test 1: Education Question (Fixed)\n",
      "================================================================================\n",
      "Query: Where did you study and what degree do you have\n",
      "================================================================================\n",
      "\n",
      "📊 Solution 1: OpenAI Embeddings + FAISS\n",
      "--------------------------------------------------------------------------------\n",
      "Relevant chunks from knowledge base (Embeddings):\n",
      "\n",
      "1. perations under time constraints\n",
      "Led public speaking and engagement efforts (guided tours, group\n",
      "presentations, marketing outreach), enhancing confidence and the ability to\n",
      "synthesize technical and no...\n",
      "\n",
      "2.  & Artificial Intelligence, Cloud Foundations \n",
      "UNIVERSITY OF BRISTOL                  BRISTOL, UNITED KINGDOM \n",
      "Bachelor of Science, Management                            SEP 2017 – JUL 2020 \n",
      "• Modules...\n",
      "\n",
      "\n",
      "⏱️  Time: 0.393s\n",
      "\n",
      "📊 Solution 2: BM25 Text Search\n",
      "--------------------------------------------------------------------------------\n",
      "Relevant chunks from knowledge base (BM25):\n",
      "\n",
      "1. deling or business recommendation and require proficiency \n",
      "in Python/ML systems.\n",
      "AI / ML Engineer CV Bullet P oint s\n",
      "1Final Recommendation: CV Narrative\n",
      "You now have a portfolio that excels across all...\n",
      "\n",
      "2. entist \n",
      "roles. If you target Data Science, focus on roles with a strong \"ML Systems\" \n",
      "or \"Advanced Modeling\" component.\n",
      "Do not target junior roles exclusively. Apply for AI/ML Engineer positions \n",
      "aski...\n",
      "\n",
      "\n",
      "⏱️  Time: 0.002s\n",
      "\n",
      "📊 Solution 3: SQLite FTS5 (Fixed)\n",
      "--------------------------------------------------------------------------------\n",
      "None\n",
      "⏱️  Time: 0.001s\n",
      "\n",
      "================================================================================\n",
      "Summary:\n",
      "Embeddings: 0.393s | BM25: 0.002s | FTS5: 0.001s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Updated comparison function using the fixed FTS5\n",
    "def compare_rag_solutions_fixed(test_query):\n",
    "    \"\"\"Compare all 3 RAG solutions on the same query - using fixed FTS5\"\"\"\n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"Query: {test_query}\")\n",
    "    print(f\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # Solution 1: Embeddings\n",
    "    print(\"📊 Solution 1: OpenAI Embeddings + FAISS\")\n",
    "    print(\"-\" * 80)\n",
    "    start = time.time()\n",
    "    result1 = search_embeddings(test_query, top_k=2)\n",
    "    time1 = time.time() - start\n",
    "    print(result1)\n",
    "    print(f\"⏱️  Time: {time1:.3f}s\")\n",
    "    print()\n",
    "    \n",
    "    # Solution 2: BM25\n",
    "    print(\"📊 Solution 2: BM25 Text Search\")\n",
    "    print(\"-\" * 80)\n",
    "    start = time.time()\n",
    "    result2 = search_bm25(test_query, top_k=2)\n",
    "    time2 = time.time() - start\n",
    "    print(result2)\n",
    "    print(f\"⏱️  Time: {time2:.3f}s\")\n",
    "    print()\n",
    "    \n",
    "    # Solution 3: FTS5 (Fixed)\n",
    "    print(\"📊 Solution 3: SQLite FTS5 (Fixed)\")\n",
    "    print(\"-\" * 80)\n",
    "    start = time.time()\n",
    "    result3 = search_fts5_fixed(test_query, top_k=2)\n",
    "    time3 = time.time() - start\n",
    "    print(result3)\n",
    "    print(f\"⏱️  Time: {time3:.3f}s\")\n",
    "    print()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Summary:\")\n",
    "    print(f\"Embeddings: {time1:.3f}s | BM25: {time2:.3f}s | FTS5: {time3:.3f}s\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Test with the fixed function\n",
    "print(\"\\n🔄 Test 1: Education Question (Fixed)\")\n",
    "compare_rag_solutions_fixed(\"Where did you study and what degree do you have\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: BM25 Text Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BM25 index...\n",
      "Created BM25 index with 71 chunks\n",
      "Relevant chunks from knowledge base (BM25):\n",
      "\n",
      "1. entist \n",
      "roles. If you target Data Science, focus on roles with a strong \"ML Systems\" \n",
      "or \"Advanced Modeling\" component.\n",
      "Do not target junior roles exclusively. Apply for AI/ML Engineer positions \n",
      "aski...\n",
      "\n",
      "2. rrent CV strength is likely at the strong Junior/Entry-Level ML/AI \n",
      "Engineer level, but with the ability to compete for roles that require 13 \n",
      "years of experience, especially if they are looking for ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution 2: BM25 Text Search\n",
    "\n",
    "print(\"Building BM25 index...\")\n",
    "chunks = chunk_documents()\n",
    "\n",
    "# Tokenize chunks (simple split on whitespace)\n",
    "tokenized_chunks = [chunk.lower().split() for chunk in chunks]\n",
    "\n",
    "# Build BM25 index\n",
    "bm25_index = BM25Okapi(tokenized_chunks)\n",
    "print(f\"Created BM25 index with {len(tokenized_chunks)} chunks\")\n",
    "\n",
    "def search_bm25(query, top_k=3):\n",
    "    \"\"\"Search using BM25 ranking\"\"\"\n",
    "    # Tokenize query\n",
    "    query_tokens = query.lower().split()\n",
    "    \n",
    "    # Get BM25 scores\n",
    "    scores = bm25_index.get_scores(query_tokens)\n",
    "    \n",
    "    # Get top-k indices\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    \n",
    "    # Get top chunks\n",
    "    results = [chunks[idx] for idx in top_indices]\n",
    "    \n",
    "    context = f\"Relevant chunks from knowledge base (BM25):\\n\\n\"\n",
    "    for i, chunk in enumerate(results, 1):\n",
    "        context += f\"{i}. {chunk[:200]}...\\n\\n\"\n",
    "    \n",
    "    return context\n",
    "\n",
    "# Test\n",
    "test_query = \"What are your technical skills?\"\n",
    "print(search_bm25(test_query, top_k=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3: SQLite FTS5 Full-Text Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating FTS5 table...\n",
      "Inserted 71 chunks into FTS5 table\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "no such column: skills",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Test\u001b[39;00m\n\u001b[32m     57\u001b[39m test_query = \u001b[33m\"\u001b[39m\u001b[33mtechnical skills\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43msearch_fts5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36msearch_fts5\u001b[39m\u001b[34m(query, top_k)\u001b[39m\n\u001b[32m     38\u001b[39m top_k_int = \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mint\u001b[39m(top_k))\n\u001b[32m     39\u001b[39m sql = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[33m    SELECT content, bm25(knowledge_base) as rank\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[33m    FROM knowledge_base\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m \u001b[33m    LIMIT \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_k_int\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m results = cursor.fetchall()\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m results:\n",
      "\u001b[31mOperationalError\u001b[39m: no such column: skills"
     ]
    }
   ],
   "source": [
    "# Solution 3: SQLite FTS5 Full-Text Search\n",
    "\n",
    "# Create FTS5 table\n",
    "cursor.execute('''\n",
    "    CREATE VIRTUAL TABLE IF NOT EXISTS knowledge_base USING fts5(\n",
    "        content,\n",
    "        source,\n",
    "        chunk_id\n",
    "    )\n",
    "''')\n",
    "\n",
    "# Populate FTS5 table\n",
    "chunks = chunk_documents()\n",
    "print(\"Populating FTS5 table...\")\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    source = \"combined_docs\"\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO knowledge_base (content, source, chunk_id) VALUES (?, ?, ?)\",\n",
    "        (chunk, source, i)\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Inserted {len(chunks)} chunks into FTS5 table\")\n",
    "\n",
    "def search_fts5(query, top_k=3):\n",
    "    \"\"\"Search using SQLite FTS5\"\"\"\n",
    "    # 1) sanitize query for FTS5 parser (remove characters that cause MATCH syntax errors)\n",
    "    cleaned = re.sub(r'[\"\\'`~*^()]+', ' ', query).replace('?', ' ').lower()\n",
    "    terms = re.findall(r\"\\w+\", cleaned)\n",
    "    if not terms:\n",
    "        return None\n",
    "    fts_expr = \" OR \".join(terms)  # e.g., technical OR skills\n",
    "\n",
    "\n",
    "    # 2) safely quote for SQL, then inline LIMIT\n",
    "    q = conn.execute(\"SELECT quote(?)\", (cleaned,)).fetchone()[0]\n",
    "    top_k_int = max(1, int(top_k))\n",
    "    sql = f\"\"\"\n",
    "        SELECT content, bm25(knowledge_base) as rank\n",
    "        FROM knowledge_base\n",
    "        WHERE knowledge_base MATCH {fts_expr}\n",
    "        ORDER BY bm25(knowledge_base)\n",
    "        LIMIT {top_k_int}\n",
    "    \"\"\"\n",
    "    cursor.execute(sql)\n",
    "    results = cursor.fetchall()\n",
    "    \n",
    "    if results:\n",
    "        context = f\"Relevant chunks from knowledge base (FTS5):\\n\\n\"\n",
    "        for i, (chunk, rank) in enumerate(results, 1):\n",
    "            context += f\"{i}. [Rank: {rank:.2f}] {chunk[:200]}...\\n\\n\"\n",
    "        return context\n",
    "    return None\n",
    "\n",
    "# Test\n",
    "test_query = \"technical skills\"\n",
    "print(search_fts5(test_query, top_k=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison: All 3 Solutions Side-by-Side\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Test 1: Education Question\n",
      "================================================================================\n",
      "Query: Where did you study and what degree do you have?\n",
      "================================================================================\n",
      "\n",
      "📊 Solution 1: OpenAI Embeddings + FAISS\n",
      "--------------------------------------------------------------------------------\n",
      "Relevant chunks from knowledge base (Embeddings):\n",
      "\n",
      "1. perations under time constraints\n",
      "Led public speaking and engagement efforts (guided tours, group\n",
      "presentations, marketing outreach), enhancing confidence and the ability to\n",
      "synthesize technical and no...\n",
      "\n",
      "2. 17 - 2020)\n",
      "  Page 4 of 5   \n",
      "Ardingly College\n",
      "International Baccalaureate  · (2015 - 2017)\n",
      "Warwick Academy\n",
      "I/GCSE's - 4 A*s, 3 A's, 1 B, 1 C  · (2013 - 2015)\n",
      "  Page 5 of 505_Master_CV_Bank.md\n",
      "Cameron B...\n",
      "\n",
      "\n",
      "⏱️  Time: 0.262s\n",
      "\n",
      "📊 Solution 2: BM25 Text Search\n",
      "--------------------------------------------------------------------------------\n",
      "Relevant chunks from knowledge base (BM25):\n",
      "\n",
      "1. entist \n",
      "roles. If you target Data Science, focus on roles with a strong \"ML Systems\" \n",
      "or \"Advanced Modeling\" component.\n",
      "Do not target junior roles exclusively. Apply for AI/ML Engineer positions \n",
      "aski...\n",
      "\n",
      "2. deling or business recommendation and require proficiency \n",
      "in Python/ML systems.\n",
      "AI / ML Engineer CV Bullet P oint s\n",
      "1Final Recommendation: CV Narrative\n",
      "You now have a portfolio that excels across all...\n",
      "\n",
      "\n",
      "⏱️  Time: 0.001s\n",
      "\n",
      "📊 Solution 3: SQLite FTS5\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "fts5: syntax error near \"?\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Test with different queries\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔄 Test 1: Education Question\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mcompare_rag_solutions\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhere did you study and what degree do you have?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🔄 Test 2: Technical Skills\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     49\u001b[39m compare_rag_solutions(\u001b[33m\"\u001b[39m\u001b[33mWhat are your technical skills and programming languages?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mcompare_rag_solutions\u001b[39m\u001b[34m(test_query)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m     31\u001b[39m start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m result3 = \u001b[43msearch_fts5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m time3 = time.time() - start\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(result3)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36msearch_fts5\u001b[39m\u001b[34m(query, top_k)\u001b[39m\n\u001b[32m     30\u001b[39m top_k_int = \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mint\u001b[39m(top_k))\n\u001b[32m     31\u001b[39m sql = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[33m    SELECT content, bm25(knowledge_base) as rank\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[33m    FROM knowledge_base\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m \u001b[33m    LIMIT \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_k_int\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m results = cursor.fetchall()\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m results:\n",
      "\u001b[31mOperationalError\u001b[39m: fts5: syntax error near \"?\""
     ]
    }
   ],
   "source": [
    "def compare_rag_solutions(test_query):\n",
    "    \"\"\"Compare all 3 RAG solutions on the same query\"\"\"\n",
    "    print(f\"=\" * 80)\n",
    "    print(f\"Query: {test_query}\")\n",
    "    print(f\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # Solution 1: Embeddings\n",
    "    print(\"📊 Solution 1: OpenAI Embeddings + FAISS\")\n",
    "    print(\"-\" * 80)\n",
    "    start = time.time()\n",
    "    result1 = search_embeddings(test_query, top_k=2)\n",
    "    time1 = time.time() - start\n",
    "    print(result1)\n",
    "    print(f\"⏱️  Time: {time1:.3f}s\")\n",
    "    print()\n",
    "    \n",
    "    # Solution 2: BM25\n",
    "    print(\"📊 Solution 2: BM25 Text Search\")\n",
    "    print(\"-\" * 80)\n",
    "    start = time.time()\n",
    "    result2 = search_bm25(test_query, top_k=2)\n",
    "    time2 = time.time() - start\n",
    "    print(result2)\n",
    "    print(f\"⏱️  Time: {time2:.3f}s\")\n",
    "    print()\n",
    "    \n",
    "    # Solution 3: FTS5\n",
    "    print(\"📊 Solution 3: SQLite FTS5\")\n",
    "    print(\"-\" * 80)\n",
    "    start = time.time()\n",
    "    result3 = search_fts5(test_query, top_k=2)\n",
    "    time3 = time.time() - start\n",
    "    print(result3)\n",
    "    print(f\"⏱️  Time: {time3:.3f}s\")\n",
    "    print()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Summary:\")\n",
    "    print(f\"Embeddings: {time1:.3f}s | BM25: {time2:.3f}s | FTS5: {time3:.3f}s\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Test with different queries\n",
    "print(\"\\n🔄 Test 1: Education Question\")\n",
    "compare_rag_solutions(\"Where did you study and what degree do you have?\")\n",
    "\n",
    "print(\"\\n\\n🔄 Test 2: Technical Skills\")\n",
    "compare_rag_solutions(\"What are your technical skills and programming languages?\")\n",
    "\n",
    "print(\"\\n\\n🔄 Test 3: Personal Background\")\n",
    "compare_rag_solutions(\"Tell me about your background and where you're from\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Solutions Summary\n",
    "\n",
    "You now have 3 working RAG implementations:\n",
    "\n",
    "1. **Solution 1 (Embeddings)**: Best semantic understanding - finds conceptually similar content even without exact keyword matches. Requires OpenAI API for embeddings (~$0.0001/1K tokens).\n",
    "\n",
    "2. **Solution 2 (BM25)**: Fast keyword-based search. Works offline, no API costs. Great for exact keyword matching.\n",
    "\n",
    "3. **Solution 3 (FTS5)**: SQL-based full-text search. Built into SQLite, persistent storage, good for structured queries.\n",
    "\n",
    "**To use in chat**: Replace `search_qa(message)` in the chat function with any of these:\n",
    "- `search_embeddings(message)` - for semantic search\n",
    "- `search_bm25(message)` - for fast keyword search  \n",
    "- `search_fts5(message)` - for SQL-based search\n",
    "\n",
    "Or combine them for hybrid retrieval!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_prompt(qa_context=\"\"):\n",
    "    system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer to any question, use your record_unknown_question tool to record the question that you couldn't answer, even if it's about something trivial or unrelated to career. \\\n",
    "If the user is engaging in discussion, try to steer them towards getting in touch via email; ask for their email and record it using your record_user_details tool. \"\n",
    "\n",
    "    system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "    \n",
    "    if qa_context:\n",
    "        system_prompt += f\"\\n{qa_context}\\n\"\n",
    "    \n",
    "    system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n",
    "    return system_prompt\n",
    "\n",
    "system_prompt = get_system_prompt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    # Search Q&A database for relevant context\n",
    "    qa_context = search_qa(message)\n",
    "    \n",
    "    # Build system prompt with Q&A context\n",
    "    prompt = get_system_prompt(qa_context)\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        # This is the call to the LLM - see that we pass in the tools json\n",
    "\n",
    "        response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, tools=tools)\n",
    "\n",
    "        finish_reason = response.choices[0].finish_reason\n",
    "        \n",
    "        # If the LLM wants to call a tool, we do that!\n",
    "         \n",
    "        if finish_reason==\"tool_calls\":\n",
    "            message = response.choices[0].message\n",
    "            tool_calls = message.tool_calls\n",
    "            results = handle_tool_calls(tool_calls)\n",
    "            messages.append(message)\n",
    "            messages.extend(results)\n",
    "        else:\n",
    "            done = True\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool called: record_unknown_question\n",
      "Push: Recording do you have a patent? asked that I couldn't answer\n",
      "Tool called: record_unknown_question\n",
      "Push: Recording Do you have a patent? asked that I couldn't answer\n",
      "Tool called: record_user_details\n",
      "Push: Recording interest from Name not provided with email cameronsobell@gmail.com and notes not provided\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And now for deployment\n",
    "\n",
    "This code is in `app.py`\n",
    "\n",
    "We will deploy to HuggingFace Spaces.\n",
    "\n",
    "Before you start: remember to update the files in the \"me\" directory - your LinkedIn profile and summary.txt - so that it talks about you! Also change `self.name = \"Ed Donner\"` in `app.py`..  \n",
    "\n",
    "Also check that there's no README file within the 1_foundations directory. If there is one, please delete it. The deploy process creates a new README file in this directory for you.\n",
    "\n",
    "1. Visit https://huggingface.co and set up an account  \n",
    "2. From the Avatar menu on the top right, choose Access Tokens. Choose \"Create New Token\". Give it WRITE permissions - it needs to have WRITE permissions! Keep a record of your new key.  \n",
    "3. In the Terminal, run: `uv tool install 'huggingface_hub[cli]'` to install the HuggingFace tool, then `hf auth login` to login at the command line with your key. Afterwards, run `hf auth whoami` to check you're logged in  \n",
    "4. Take your new token and add it to your .env file: `HF_TOKEN=hf_xxx` for the future\n",
    "5. From the 1_foundations folder, enter: `uv run gradio deploy` \n",
    "6. Follow its instructions: name it \"career_conversation\", specify app.py, choose cpu-basic as the hardware, say Yes to needing to supply secrets, provide your openai api key, your pushover user and token, and say \"no\" to github actions.  \n",
    "\n",
    "Thank you Robert, James, Martins, Andras and Priya for these tips.  \n",
    "Please read the next 2 sections - how to change your Secrets, and how to redeploy your Space (you may need to delete the README.md that gets created in this 1_foundations directory).\n",
    "\n",
    "#### More about these secrets:\n",
    "\n",
    "If you're confused by what's going on with these secrets: it just wants you to enter the key name and value for each of your secrets -- so you would enter:  \n",
    "`OPENAI_API_KEY`  \n",
    "Followed by:  \n",
    "`sk-proj-...`  \n",
    "\n",
    "And if you don't want to set secrets this way, or something goes wrong with it, it's no problem - you can change your secrets later:  \n",
    "1. Log in to HuggingFace website  \n",
    "2. Go to your profile screen via the Avatar menu on the top right  \n",
    "3. Select the Space you deployed  \n",
    "4. Click on the Settings wheel on the top right  \n",
    "5. You can scroll down to change your secrets (Variables and Secrets section), delete the space, etc.\n",
    "\n",
    "#### And now you should be deployed!\n",
    "\n",
    "If you want to completely replace everything and start again with your keys, you may need to delete the README.md that got created in this 1_foundations folder.\n",
    "\n",
    "Here is mine: https://huggingface.co/spaces/ed-donner/Career_Conversation\n",
    "\n",
    "I just got a push notification that a student asked me how they can become President of their country 😂😂\n",
    "\n",
    "For more information on deployment:\n",
    "\n",
    "https://www.gradio.app/guides/sharing-your-app#hosting-on-hf-spaces\n",
    "\n",
    "To delete your Space in the future:  \n",
    "1. Log in to HuggingFace\n",
    "2. From the Avatar menu, select your profile\n",
    "3. Click on the Space itself and select the settings wheel on the top right\n",
    "4. Scroll to the Delete section at the bottom\n",
    "5. ALSO: delete the README file that Gradio may have created inside this 1_foundations folder (otherwise it won't ask you the questions the next time you do a gradio deploy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">• First and foremost, deploy this for yourself! It's a real, valuable tool - the future resume..<br/>\n",
    "            • Next, improve the resources - add better context about yourself. If you know RAG, then add a knowledge base about you.<br/>\n",
    "            • Add in more tools! You could have a SQL database with common Q&A that the LLM could read and write from?<br/>\n",
    "            • Bring in the Evaluator from the last lab, and add other Agentic patterns.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">Aside from the obvious (your career alter-ego) this has business applications in any situation where you need an AI assistant with domain expertise and an ability to interact with the real world.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
